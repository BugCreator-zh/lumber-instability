{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from config import Config\n",
    "from models.multiple_input_net import MIN\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.read_data import load_dataset\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    grad_block.append(grad_out[0].detach())\n",
    "\n",
    "def farward_hook(module, input, output):\n",
    "    fmap_block.append(output)\n",
    "\n",
    "def comp_class_vec(ouput_vec, index=None):\n",
    "    \"\"\"\n",
    "    计算类向量\n",
    "    :param ouput_vec: tensor\n",
    "    :param index: int，指定类别\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    if not index:\n",
    "        index = np.argmax(ouput_vec.cpu().data.numpy())\n",
    "    else:\n",
    "        index = np.array(index)\n",
    "    index = index[np.newaxis, np.newaxis]\n",
    "    index = torch.from_numpy(index)\n",
    "    one_hot = torch.zeros(1, 2).scatter_(1, index, 1)\n",
    "    one_hot.requires_grad = True\n",
    "    class_vec = torch.sum(one_hot * ouput_vec)  # one_hot = 11.8605\n",
    "\n",
    "    return class_vec\n",
    "    \n",
    "def gen_cam(feature_map, grads,H,W):\n",
    "    \"\"\"\n",
    "    依据梯度和特征图，生成cam\n",
    "    :param feature_map: np.array， in [C, H, W]\n",
    "    :param grads: np.array， in [C, H, W]\n",
    "    :return: np.array, [H, W]\n",
    "    \"\"\"\n",
    "    cam = np.zeros(feature_map.shape[1:], dtype=np.float32)  # cam shape (H, W)\n",
    "\n",
    "    weights = np.mean(grads, axis=(1, 2))  #\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * feature_map[i, :, :]\n",
    "\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cv2.resize(cam, (H, W))\n",
    "    return cam\n",
    "\n",
    "def show_cam_on_image(img, mask, out_dir,index):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    \n",
    "    path_cam_img = os.path.join(out_dir, index+\"_cam.jpg\")\n",
    "    path_raw_img = os.path.join(out_dir, index+\"_raw.jpg\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    cv2.imwrite(path_cam_img, np.uint8(255 * cam))\n",
    "    cv2.imwrite(path_raw_img, np.uint8(255 * img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_8  missing SAG\n",
      "unlabelled_0310  missing SAG\n",
      "unlabelled_0365  missing TRA\n",
      "unlabelled_0375  missing TRA\n",
      "unlabelled_0401  missing SAG\n",
      "unlabelled_0447  missing TRA\n",
      "unlabelled_0468  missing TRA\n",
      "Train negative samples: 10\n",
      "Train positive samples: 75\n",
      "Test negative samples: 6\n",
      "Test positive samples: 38\n"
     ]
    }
   ],
   "source": [
    "config = Config.Config()\n",
    "config.device = 'cpu'\n",
    "model_path = 'logs//best_trained_model'\n",
    "model = MIN(config,['alexnet','alexnet']).to(config.device)\n",
    "model.load_state_dict(torch.load(model_path,map_location=config.device))\n",
    "model.sag_model.last_conv.register_forward_hook(farward_hook)\n",
    "model.sag_model.last_conv.register_backward_hook(backward_hook)\n",
    "#model.tra_model.last_conv.register_forward_hook(farward_hook)\n",
    "#model.tra_model.last_conv.register_backward_hook(backward_hook)\n",
    "sag_list = ['L3-L4','L4-L5','L5-S1']\n",
    "train_data,test_data = load_dataset(config,sag_list,train_split = 2/3,from_trained = True) \n",
    "train_data = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_data = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单次生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward&backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::values' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [SparseCPU, SparseCUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten\\src\\ATen\\RegisterSparseCPU.cpp:557 [kernel]\nSparseCUDA: registered at aten\\src\\ATen\\RegisterSparseCUDA.cpp:655 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_1.cpp:11324 [kernel]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14940/371936959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgrad_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msample_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#y = y.to(config.device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not run 'aten::values' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::values' is only available for these backends: [SparseCPU, SparseCUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nSparseCPU: registered at aten\\src\\ATen\\RegisterSparseCPU.cpp:557 [kernel]\nSparseCUDA: registered at aten\\src\\ATen\\RegisterSparseCUDA.cpp:655 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:9509 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_1.cpp:11324 [kernel]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:250 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "fmap_block = list()\n",
    "grad_block = list()\n",
    "sample_id,(tra,sag), y = test_data.dataset.__getitem__(0)\n",
    "x1 = [i.to(config.device).unsqueeze(0) for i in sag.values()]\n",
    "x2 = tra.to(config.device).unsqueeze(0)\n",
    "#y = y.to(config.device)\n",
    "model.eval()\n",
    "outputs = model(x1,x2)\n",
    "loss = comp_class_vec(outputs)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sag = {}\n",
    "sag_list = ['L3-L4','L4-L5','L5-S1']\n",
    "img_path = test_data.dataset.samples[0][0]\n",
    "for x in os.listdir(img_path + '//' + 'SAG'):\n",
    "    if x[0:5] in sag_list:\n",
    "        img = cv2.imread(img_path + '//' + 'SAG'+'//'+x,1)\n",
    "        sag[x[0:5]] = img\n",
    "tra = cv2.imread(img_path + '//' + 'TRA'+'//'+ os.listdir(img_path + '//' + 'TRA')[0],1)\n",
    "output_dir = 'attention_maps//'\n",
    "idx = ['tra']+list(sag.keys())\n",
    "graphs = [tra] + list(sag.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,graph in enumerate(graphs):\n",
    "    grads = np.array(grad_block[i][0].cpu())\n",
    "    fmap = np.array(fmap_block[i][0].cpu().detach())\n",
    "    H = len(graph[0])\n",
    "    W = len(graph)\n",
    "    cam = gen_cam(fmap, grads,H,W)\n",
    "    img_show = np.float32(cv2.resize(img, (H,W))) / 255\n",
    "    show_cam_on_image(img_show, cam, output_dir+str(sample_id),idx[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------0.0%\n",
      "------------0.22522522522522523%\n",
      "------------0.45045045045045046%\n",
      "------------0.6756756756756757%\n",
      "------------0.9009009009009009%\n",
      "------------1.1261261261261262%\n",
      "------------1.3513513513513513%\n",
      "------------1.5765765765765765%\n",
      "------------1.8018018018018018%\n",
      "------------2.027027027027027%\n",
      "------------2.2522522522522523%\n",
      "------------2.4774774774774775%\n",
      "------------2.7027027027027026%\n",
      "------------2.9279279279279278%\n",
      "------------3.153153153153153%\n",
      "------------3.3783783783783785%\n",
      "------------3.6036036036036037%\n",
      "------------3.8288288288288284%\n",
      "------------4.054054054054054%\n",
      "------------4.2792792792792795%\n",
      "------------4.504504504504505%\n",
      "------------4.72972972972973%\n",
      "------------4.954954954954955%\n",
      "------------5.18018018018018%\n",
      "------------5.405405405405405%\n",
      "------------5.63063063063063%\n",
      "------------5.8558558558558556%\n",
      "------------6.081081081081082%\n",
      "------------6.306306306306306%\n",
      "------------6.531531531531531%\n",
      "------------6.756756756756757%\n",
      "------------6.981981981981981%\n",
      "------------7.207207207207207%\n",
      "------------7.4324324324324325%\n",
      "------------7.657657657657657%\n",
      "------------7.882882882882883%\n",
      "------------8.108108108108109%\n",
      "------------8.333333333333332%\n",
      "------------8.558558558558559%\n",
      "------------8.783783783783784%\n",
      "------------9.00900900900901%\n",
      "------------9.234234234234235%\n",
      "------------9.45945945945946%\n",
      "------------9.684684684684685%\n",
      "------------9.90990990990991%\n",
      "------------10.135135135135135%\n",
      "------------10.36036036036036%\n",
      "------------10.585585585585585%\n",
      "------------10.81081081081081%\n",
      "------------11.036036036036036%\n",
      "------------11.26126126126126%\n",
      "------------11.486486486486488%\n",
      "------------11.711711711711711%\n",
      "------------11.936936936936938%\n",
      "------------12.162162162162163%\n",
      "------------12.387387387387387%\n",
      "------------12.612612612612612%\n",
      "------------12.837837837837837%\n",
      "------------13.063063063063062%\n",
      "------------13.288288288288289%\n",
      "------------13.513513513513514%\n",
      "------------13.73873873873874%\n",
      "------------13.963963963963963%\n",
      "------------14.18918918918919%\n",
      "------------14.414414414414415%\n",
      "------------14.63963963963964%\n",
      "------------14.864864864864865%\n",
      "------------15.090090090090092%\n",
      "------------15.315315315315313%\n",
      "------------15.54054054054054%\n",
      "------------15.765765765765765%\n",
      "------------15.99099099099099%\n",
      "------------16.216216216216218%\n",
      "------------16.441441441441444%\n",
      "------------16.666666666666664%\n",
      "------------16.89189189189189%\n",
      "------------17.117117117117118%\n",
      "------------17.34234234234234%\n",
      "------------17.56756756756757%\n",
      "------------17.792792792792792%\n",
      "------------18.01801801801802%\n",
      "------------18.243243243243242%\n",
      "------------18.46846846846847%\n",
      "------------18.693693693693696%\n",
      "------------18.91891891891892%\n",
      "------------19.144144144144143%\n",
      "------------19.36936936936937%\n",
      "------------19.594594594594593%\n",
      "------------19.81981981981982%\n",
      "------------20.045045045045047%\n",
      "------------20.27027027027027%\n",
      "------------20.495495495495494%\n",
      "------------20.72072072072072%\n",
      "------------20.945945945945947%\n",
      "------------21.17117117117117%\n",
      "------------21.396396396396398%\n",
      "------------21.62162162162162%\n",
      "------------21.846846846846844%\n",
      "------------22.07207207207207%\n",
      "------------22.2972972972973%\n",
      "------------22.52252252252252%\n",
      "------------22.74774774774775%\n",
      "------------22.972972972972975%\n",
      "------------23.1981981981982%\n",
      "------------23.423423423423422%\n",
      "------------23.64864864864865%\n",
      "------------23.873873873873876%\n",
      "------------24.0990990990991%\n",
      "------------24.324324324324326%\n",
      "------------24.54954954954955%\n",
      "------------24.774774774774773%\n",
      "------------25.0%\n",
      "------------25.225225225225223%\n",
      "------------25.45045045045045%\n",
      "------------25.675675675675674%\n",
      "------------25.900900900900904%\n",
      "------------26.126126126126124%\n",
      "------------26.351351351351347%\n",
      "------------26.576576576576578%\n",
      "------------26.8018018018018%\n",
      "------------27.027027027027028%\n",
      "------------27.25225225225225%\n",
      "------------27.47747747747748%\n",
      "------------27.7027027027027%\n",
      "------------27.927927927927925%\n",
      "------------28.153153153153156%\n",
      "------------28.37837837837838%\n",
      "------------28.603603603603606%\n",
      "------------28.82882882882883%\n",
      "------------29.054054054054053%\n",
      "------------29.27927927927928%\n",
      "------------29.504504504504503%\n",
      "------------29.72972972972973%\n",
      "------------29.954954954954953%\n",
      "------------30.180180180180184%\n",
      "------------30.405405405405407%\n",
      "------------30.630630630630627%\n",
      "------------30.855855855855857%\n",
      "------------31.08108108108108%\n",
      "------------31.306306306306308%\n",
      "------------31.53153153153153%\n",
      "------------31.756756756756754%\n",
      "------------31.98198198198198%\n",
      "------------32.207207207207205%\n",
      "------------32.432432432432435%\n",
      "------------32.65765765765766%\n",
      "------------32.88288288288289%\n",
      "------------33.108108108108105%\n",
      "------------33.33333333333333%\n",
      "------------33.55855855855856%\n",
      "------------33.78378378378378%\n",
      "------------34.00900900900901%\n",
      "------------34.234234234234236%\n",
      "------------34.45945945945946%\n",
      "------------34.68468468468468%\n",
      "------------34.909909909909906%\n",
      "------------35.13513513513514%\n",
      "------------35.36036036036036%\n",
      "------------35.585585585585584%\n",
      "------------35.810810810810814%\n",
      "------------36.03603603603604%\n",
      "------------36.26126126126126%\n",
      "------------36.486486486486484%\n",
      "------------36.711711711711715%\n",
      "------------36.93693693693694%\n",
      "------------37.16216216216216%\n",
      "------------37.38738738738739%\n",
      "------------37.61261261261261%\n",
      "------------37.83783783783784%\n",
      "------------38.06306306306306%\n",
      "------------38.288288288288285%\n",
      "------------38.513513513513516%\n",
      "------------38.73873873873874%\n",
      "------------38.96396396396396%\n",
      "------------39.189189189189186%\n",
      "------------39.414414414414416%\n",
      "------------39.63963963963964%\n",
      "------------39.86486486486486%\n",
      "------------40.090090090090094%\n",
      "------------40.31531531531532%\n",
      "------------40.54054054054054%\n",
      "------------40.765765765765764%\n",
      "------------40.99099099099099%\n",
      "------------41.21621621621622%\n",
      "------------41.44144144144144%\n",
      "------------41.66666666666667%\n",
      "------------41.891891891891895%\n",
      "------------42.11711711711711%\n",
      "------------42.34234234234234%\n",
      "------------42.567567567567565%\n",
      "------------42.792792792792795%\n",
      "------------43.01801801801802%\n",
      "------------43.24324324324324%\n",
      "------------43.468468468468465%\n",
      "------------43.69369369369369%\n",
      "------------43.91891891891892%\n",
      "------------44.14414414414414%\n",
      "------------44.36936936936937%\n",
      "------------44.5945945945946%\n",
      "------------44.81981981981982%\n",
      "------------45.04504504504504%\n",
      "------------45.27027027027027%\n",
      "------------45.4954954954955%\n",
      "------------45.72072072072072%\n",
      "------------45.94594594594595%\n",
      "------------46.171171171171174%\n",
      "------------46.3963963963964%\n",
      "------------46.62162162162162%\n",
      "------------46.846846846846844%\n",
      "------------47.072072072072075%\n",
      "------------47.2972972972973%\n",
      "------------47.52252252252252%\n",
      "------------47.74774774774775%\n",
      "------------47.97297297297297%\n",
      "------------48.1981981981982%\n",
      "------------48.42342342342342%\n",
      "------------48.64864864864865%\n",
      "------------48.873873873873876%\n",
      "------------49.0990990990991%\n",
      "------------49.32432432432432%\n",
      "------------49.549549549549546%\n",
      "------------49.77477477477478%\n",
      "------------50.0%\n",
      "------------50.22522522522522%\n",
      "------------50.45045045045045%\n",
      "------------50.67567567567568%\n",
      "------------50.9009009009009%\n",
      "------------51.126126126126124%\n",
      "------------51.35135135135135%\n",
      "------------51.57657657657657%\n",
      "------------51.80180180180181%\n",
      "------------52.02702702702703%\n",
      "------------52.25225225225225%\n",
      "------------52.47747747747747%\n",
      "------------52.702702702702695%\n",
      "------------52.92792792792793%\n",
      "------------53.153153153153156%\n",
      "------------53.37837837837838%\n",
      "------------53.6036036036036%\n",
      "------------53.82882882882883%\n",
      "------------54.054054054054056%\n",
      "------------54.27927927927928%\n",
      "------------54.5045045045045%\n",
      "------------54.729729729729726%\n",
      "------------54.95495495495496%\n",
      "------------55.18018018018018%\n",
      "------------55.4054054054054%\n",
      "------------55.63063063063063%\n",
      "------------55.85585585585585%\n",
      "------------56.08108108108109%\n",
      "------------56.30630630630631%\n",
      "------------56.531531531531535%\n",
      "------------56.75675675675676%\n",
      "------------56.981981981981974%\n",
      "------------57.20720720720721%\n",
      "------------57.432432432432435%\n",
      "------------57.65765765765766%\n",
      "------------57.88288288288288%\n",
      "------------58.108108108108105%\n",
      "------------58.333333333333336%\n",
      "------------58.55855855855856%\n",
      "------------58.78378378378378%\n",
      "------------59.009009009009006%\n",
      "------------59.23423423423423%\n",
      "------------59.45945945945946%\n",
      "------------59.68468468468468%\n",
      "------------59.909909909909906%\n",
      "------------60.13513513513513%\n",
      "------------60.36036036036037%\n",
      "------------60.58558558558559%\n",
      "------------60.810810810810814%\n",
      "------------61.03603603603604%\n",
      "------------61.261261261261254%\n",
      "------------61.48648648648649%\n",
      "------------61.711711711711715%\n",
      "------------61.93693693693694%\n",
      "------------62.16216216216216%\n",
      "------------62.387387387387385%\n",
      "------------62.612612612612615%\n",
      "------------62.83783783783784%\n",
      "------------63.06306306306306%\n",
      "------------63.288288288288285%\n",
      "------------63.51351351351351%\n",
      "------------63.738738738738746%\n",
      "------------63.96396396396396%\n",
      "------------64.1891891891892%\n",
      "------------64.41441441441441%\n",
      "------------64.63963963963964%\n",
      "------------64.86486486486487%\n",
      "------------65.09009009009009%\n",
      "------------65.31531531531532%\n",
      "------------65.54054054054053%\n",
      "------------65.76576576576578%\n",
      "------------65.990990990991%\n",
      "------------66.21621621621621%\n",
      "------------66.44144144144144%\n",
      "------------66.66666666666666%\n",
      "------------66.8918918918919%\n",
      "------------67.11711711711712%\n",
      "------------67.34234234234235%\n",
      "------------67.56756756756756%\n",
      "------------67.7927927927928%\n",
      "------------68.01801801801803%\n",
      "------------68.24324324324324%\n",
      "------------68.46846846846847%\n",
      "------------68.69369369369369%\n",
      "------------68.91891891891892%\n",
      "------------69.14414414414415%\n",
      "------------69.36936936936937%\n",
      "------------69.5945945945946%\n",
      "------------69.81981981981981%\n",
      "------------70.04504504504504%\n",
      "------------70.27027027027027%\n",
      "------------70.4954954954955%\n",
      "------------70.72072072072072%\n",
      "------------70.94594594594594%\n",
      "------------71.17117117117117%\n",
      "------------71.3963963963964%\n",
      "------------71.62162162162163%\n",
      "------------71.84684684684684%\n",
      "------------72.07207207207207%\n",
      "------------72.2972972972973%\n",
      "------------72.52252252252252%\n",
      "------------72.74774774774775%\n",
      "------------72.97297297297297%\n",
      "------------73.1981981981982%\n",
      "------------73.42342342342343%\n",
      "------------73.64864864864865%\n",
      "------------73.87387387387388%\n",
      "------------74.09909909909909%\n",
      "------------74.32432432432432%\n",
      "------------74.54954954954955%\n",
      "------------74.77477477477478%\n",
      "------------75.0%\n",
      "------------75.22522522522522%\n",
      "------------75.45045045045045%\n",
      "------------75.67567567567568%\n",
      "------------75.90090090090091%\n",
      "------------76.12612612612612%\n",
      "------------76.35135135135135%\n",
      "------------76.57657657657657%\n",
      "------------76.8018018018018%\n",
      "------------77.02702702702703%\n",
      "------------77.25225225225225%\n",
      "------------77.47747747747748%\n",
      "------------77.7027027027027%\n",
      "------------77.92792792792793%\n",
      "------------78.15315315315316%\n",
      "------------78.37837837837837%\n",
      "------------78.6036036036036%\n",
      "------------78.82882882882883%\n",
      "------------79.05405405405406%\n",
      "------------79.27927927927928%\n",
      "------------79.5045045045045%\n",
      "------------79.72972972972973%\n",
      "------------79.95495495495496%\n",
      "------------80.18018018018019%\n",
      "------------80.4054054054054%\n",
      "------------80.63063063063063%\n",
      "------------80.85585585585585%\n",
      "------------81.08108108108108%\n",
      "------------81.30630630630631%\n",
      "------------81.53153153153153%\n",
      "------------81.75675675675676%\n",
      "------------81.98198198198197%\n",
      "------------82.2072072072072%\n",
      "------------82.43243243243244%\n",
      "------------82.65765765765765%\n",
      "------------82.88288288288288%\n",
      "------------83.1081081081081%\n",
      "------------83.33333333333334%\n",
      "------------83.55855855855856%\n",
      "------------83.78378378378379%\n",
      "------------84.009009009009%\n",
      "------------84.23423423423422%\n",
      "------------84.45945945945947%\n",
      "------------84.68468468468468%\n",
      "------------84.90990990990991%\n",
      "------------85.13513513513513%\n",
      "------------85.36036036036036%\n",
      "------------85.58558558558559%\n",
      "------------85.8108108108108%\n",
      "------------86.03603603603604%\n",
      "------------86.26126126126125%\n",
      "------------86.48648648648648%\n",
      "------------86.71171171171171%\n",
      "------------86.93693693693693%\n",
      "------------87.16216216216216%\n",
      "------------87.38738738738738%\n",
      "------------87.61261261261262%\n",
      "------------87.83783783783784%\n",
      "------------88.06306306306307%\n",
      "------------88.28828828828829%\n",
      "------------88.51351351351352%\n",
      "------------88.73873873873875%\n",
      "------------88.96396396396396%\n",
      "------------89.1891891891892%\n",
      "------------89.41441441441441%\n",
      "------------89.63963963963964%\n",
      "------------89.86486486486487%\n",
      "------------90.09009009009009%\n",
      "------------90.31531531531532%\n",
      "------------90.54054054054053%\n",
      "------------90.76576576576578%\n",
      "------------90.990990990991%\n",
      "------------91.21621621621621%\n",
      "------------91.44144144144144%\n",
      "------------91.66666666666666%\n",
      "------------91.8918918918919%\n",
      "------------92.11711711711712%\n",
      "------------92.34234234234235%\n",
      "------------92.56756756756756%\n",
      "------------92.7927927927928%\n",
      "------------93.01801801801803%\n",
      "------------93.24324324324324%\n",
      "------------93.46846846846847%\n",
      "------------93.69369369369369%\n",
      "------------93.91891891891892%\n",
      "------------94.14414414414415%\n",
      "------------94.36936936936937%\n",
      "------------94.5945945945946%\n",
      "------------94.81981981981981%\n",
      "------------95.04504504504504%\n",
      "------------95.27027027027027%\n",
      "------------95.4954954954955%\n",
      "------------95.72072072072072%\n",
      "------------95.94594594594594%\n",
      "------------96.17117117117117%\n",
      "------------96.3963963963964%\n",
      "------------96.62162162162163%\n",
      "------------96.84684684684684%\n",
      "------------97.07207207207207%\n",
      "------------97.2972972972973%\n",
      "------------97.52252252252252%\n",
      "------------97.74774774774775%\n",
      "------------97.97297297297297%\n",
      "------------98.1981981981982%\n",
      "------------98.42342342342343%\n",
      "------------98.64864864864865%\n",
      "------------98.87387387387388%\n",
      "------------99.09909909909909%\n",
      "------------99.32432432432432%\n",
      "------------99.54954954954955%\n",
      "------------99.77477477477478%\n"
     ]
    }
   ],
   "source": [
    "num = len(test_data.dataset)\n",
    "for item_idx in range(num):\n",
    "    print('------------'+str(item_idx/num*100)+'%')\n",
    "    fmap_block = list()\n",
    "    grad_block = list()\n",
    "    sample_id,(tra,sag), y = test_data.dataset.__getitem__(item_idx)\n",
    "    if y == 2:\n",
    "        continue\n",
    "    x1 = [sag.to(config.device)]\n",
    "    x2 = [tra.to(config.device)]\n",
    "    #y = y.to(config.device)\n",
    "    model.eval()\n",
    "    outputs = model(x1,x2)\n",
    "    loss = comp_class_vec(outputs,np.array([1],dtype = 'int64')[0])\n",
    "    loss.backward()\n",
    "\n",
    "    sag = {}\n",
    "    sag_list = ['L3-L4','L4-L5','L5-S1']\n",
    "    img_path = test_data.dataset.samples[item_idx][0]\n",
    "    for x in os.listdir(img_path + '//' + 'SAG'):\n",
    "        if x[0:5] in sag_list:\n",
    "            img = cv2.imread(img_path + '//' + 'SAG'+'//'+x,1)\n",
    "            sag[x[0:5]] = img\n",
    "    tra = cv2.imread(img_path + '//' + 'TRA'+'//'+ os.listdir(img_path + '//' + 'TRA')[0],1)\n",
    "    output_dir = 'attention_maps//'\n",
    "    idx = ['tra']+list(sag.keys())\n",
    "    graphs = [tra] + list(sag.values())\n",
    "\n",
    "    cam = list(range(len(idx)))\n",
    "    min_cam = 100\n",
    "    max_cam = 0\n",
    "    for i,graph in enumerate(graphs):\n",
    "        grads = np.array(grad_block[i][0].cpu())\n",
    "        fmap = np.array(fmap_block[i][0].cpu().detach())\n",
    "        H = len(graph[0])\n",
    "        W = len(graph)\n",
    "        cam[i] = gen_cam(fmap, grads,H,W)\n",
    "        if np.min(cam[i]) < min_cam:\n",
    "            min_cam = np.min(cam[i])\n",
    "        if np.max(cam[i]) > max_cam:\n",
    "            max_cam = np.max(cam[i])\n",
    "         \n",
    "    for i in range(len(cam)):\n",
    "        cam[i] -= min_cam\n",
    "        if max_cam!=0:\n",
    "            cam[i] /= max_cam\n",
    "\n",
    "    for i,graph in enumerate(graphs):\n",
    "        H = len(graph[0])\n",
    "        W = len(graph)\n",
    "        img_show = np.float32(cv2.resize(graph, (H,W))) / 255\n",
    "        show_cam_on_image(img_show, cam[i], output_dir+str(sample_id),idx[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a069246156d2e03731e389a0480115824d4d29d06cd23929b1fd90c7099d555b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
